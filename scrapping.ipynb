{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "  \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def scroll_to_load_jobs(driver, target_jobs=200, scroll_pause_time=1.5):\n",
    "    \n",
    "    print(f\"  Scrolling to load up to {target_jobs} jobs...\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    jobs_loaded = 0\n",
    "    scroll_attempts = 0\n",
    "    max_attempts = 20\n",
    "    consecutive_no_new_jobs = 0\n",
    "\n",
    "    while jobs_loaded < target_jobs and scroll_attempts < max_attempts:\n",
    "        scroll_attempts += 1\n",
    "\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(scroll_pause_time)\n",
    "\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "\n",
    "       \n",
    "        try:\n",
    "            current_jobs = driver.find_elements(By.CSS_SELECTOR, \"tr.job\")\n",
    "            new_jobs_loaded = len(current_jobs)\n",
    "        except StaleElementReferenceException:\n",
    "            current_jobs = driver.find_elements(By.CSS_SELECTOR, \"tr.job\")\n",
    "            new_jobs_loaded = len(current_jobs)\n",
    "\n",
    "        \n",
    "        if new_jobs_loaded > jobs_loaded:\n",
    "            jobs_loaded = new_jobs_loaded\n",
    "            consecutive_no_new_jobs = 0\n",
    "            if jobs_loaded % 50 == 0:\n",
    "                print(f\"    Loaded {jobs_loaded} jobs...\")\n",
    "        else:\n",
    "            consecutive_no_new_jobs += 1\n",
    "\n",
    "       \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            if consecutive_no_new_jobs > 2:\n",
    "                print(f\"    Reached bottom. Found {jobs_loaded} jobs.\")\n",
    "                break\n",
    "        else:\n",
    "            last_height = new_height\n",
    "\n",
    "        \n",
    "        if jobs_loaded >= target_jobs:\n",
    "            break\n",
    "\n",
    "    print(f\"    Total loaded from this page: {jobs_loaded} jobs\")\n",
    "    return jobs_loaded\n",
    "\n",
    "\n",
    "def extract_location_details(location_text):\n",
    "    \n",
    "    country = \"\"\n",
    "    city = \"\"\n",
    "\n",
    "    if not location_text or location_text.lower() in ['remote', 'anywhere', 'global', 'worldwide']:\n",
    "        country = \"Remote\"\n",
    "        city = \"Remote\"\n",
    "    else:\n",
    "        location_text = location_text.strip()\n",
    "\n",
    "       \n",
    "        timezone_patterns = ['est', 'pst', 'cst', 'mst', 'gmt', 'utc']\n",
    "        for tz in timezone_patterns:\n",
    "            if tz in location_text.lower():\n",
    "                country = f\"{tz.upper()} Timezone\"\n",
    "                city = location_text\n",
    "                return country, city\n",
    "\n",
    "        \n",
    "        countries = ['usa', 'united states', 'us', 'uk', 'united kingdom', 'canada',\n",
    "                     'germany', 'france', 'spain', 'italy', 'india', 'australia',\n",
    "                     'netherlands', 'sweden', 'denmark', 'norway', 'finland',\n",
    "                     'brazil', 'mexico', 'argentina', 'singapore', 'japan']\n",
    "\n",
    "        for c in countries:\n",
    "            if c in location_text.lower():\n",
    "                country = c.title()\n",
    "                parts = re.split(r'[,|\\-]', location_text, flags=re.IGNORECASE)\n",
    "                if len(parts) > 1:\n",
    "                    city = parts[0].strip()\n",
    "                else:\n",
    "                    city = re.sub(c, '', location_text, flags=re.IGNORECASE).strip()\n",
    "                    if not city:\n",
    "                        city = \"Multiple locations\"\n",
    "                break\n",
    "\n",
    "        if not country:\n",
    "            country = location_text\n",
    "            city = \"Multiple locations\"\n",
    "\n",
    "    return country, city\n",
    "\n",
    "\n",
    "def categorize_job(title, tags):\n",
    "    \n",
    "    title_lower = title.lower()\n",
    "    tags_lower = ' '.join([tag.lower() for tag in tags]) if tags else \"\"\n",
    "\n",
    "    categories = {\n",
    "        'Data Analysis': ['data analyst', 'data analysis', 'analytics', 'bi analyst',\n",
    "                          'business intelligence', 'sql', 'tableau', 'power bi', 'excel'],\n",
    "        'Data Science': ['data scientist', 'data science', 'machine learning', 'ml',\n",
    "                         'ai', 'artificial intelligence', 'deep learning', 'nlp'],\n",
    "        'Software Development': ['software engineer', 'developer', 'programmer',\n",
    "                                 'full stack', 'frontend', 'backend', 'web developer'],\n",
    "        'DevOps': ['devops', 'sre', 'site reliability', 'cloud engineer', 'aws',\n",
    "                   'azure', 'docker', 'kubernetes'],\n",
    "        'Design': ['designer', 'ui/ux', 'ui ', 'ux ', 'graphic design', 'product design'],\n",
    "        'Marketing': ['marketing', 'seo', 'sem', 'digital marketing', 'growth hacker'],\n",
    "        'Sales': ['sales', 'account executive', 'business development', 'bd'],\n",
    "        'Support': ['support', 'customer success', 'help desk', 'technical support'],\n",
    "        'Product Management': ['product manager', 'product owner', 'pm ', 'product management'],\n",
    "        'Finance': ['finance', 'accounting', 'financial analyst', 'cpa', 'accountant'],\n",
    "        'QA/Testing': ['qa', 'quality assurance', 'test engineer', 'automation testing'],\n",
    "        'Project Management': ['project manager', 'scrum master', 'agile'],\n",
    "        'Content Writing': ['content writer', 'copywriter', 'technical writer', 'blogger']\n",
    "    }\n",
    "\n",
    "    for category, keywords in categories.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in title_lower or keyword in tags_lower:\n",
    "                return category\n",
    "\n",
    "    return 'Other'\n",
    "\n",
    "\n",
    "def extract_skills_from_tags(tags):\n",
    "    \n",
    "    if not tags:\n",
    "        return []\n",
    "\n",
    "    skill_categories = {\n",
    "        'Programming Languages': [\n",
    "            'python', 'javascript', 'java', 'typescript', 'c#', 'c++', 'ruby',\n",
    "            'php', 'go', 'rust', 'swift', 'kotlin', 'scala', 'r', 'matlab'\n",
    "        ],\n",
    "        'Frontend Technologies': [\n",
    "            'react', 'angular', 'vue', 'next.js', 'nuxt.js', 'svelte',\n",
    "            'html', 'css', 'sass', 'less', 'tailwind', 'bootstrap', 'jquery'\n",
    "        ],\n",
    "        'Backend Frameworks': [\n",
    "            'node.js', 'django', 'flask', 'spring', 'express.js', 'laravel',\n",
    "            'rails', 'asp.net', 'fastapi', 'nestjs', 'graphql'\n",
    "        ],\n",
    "        'Databases': [\n",
    "            'sql', 'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',\n",
    "            'oracle', 'sql server', 'cassandra', 'dynamodb', 'firebase'\n",
    "        ],\n",
    "        'Cloud & DevOps': [\n",
    "            'aws', 'azure', 'gcp', 'docker', 'kubernetes', 'terraform',\n",
    "            'ansible', 'jenkins', 'ci/cd', 'github actions', 'gitlab ci'\n",
    "        ],\n",
    "        'Data Science & ML': [\n",
    "            'pandas', 'numpy', 'scikit-learn', 'tensorflow', 'pytorch', 'keras',\n",
    "            'spark', 'hadoop', 'hive', 'airflow', 'tableau', 'power bi', 'looker'\n",
    "        ],\n",
    "        'Tools & Platforms': [\n",
    "            'git', 'github', 'gitlab', 'jira', 'confluence', 'figma', 'slack',\n",
    "            'trello', 'asana', 'notion', 'wordpress', 'shopify', 'salesforce'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    tags_lower = [tag.lower() for tag in tags]\n",
    "    skills_found = []\n",
    "\n",
    "    for category, skill_list in skill_categories.items():\n",
    "        for skill in skill_list:\n",
    "            for tag in tags_lower:\n",
    "                skill_variations = [skill]\n",
    "                if skill.endswith('.js'):\n",
    "                    skill_variations.append(skill.replace('.js', ''))\n",
    "\n",
    "                for skill_var in skill_variations:\n",
    "                    if skill_var in tag:\n",
    "                        formatted_skill = skill.title() if '.' not in skill else skill\n",
    "                        skills_found.append(formatted_skill)\n",
    "                        break\n",
    "\n",
    "    for tag in tags_lower:\n",
    "        if any(lang in tag for lang in ['english', 'spanish', 'french', 'german', 'chinese']):\n",
    "            skills_found.append(tag.title())\n",
    "\n",
    "    skills_found = list(dict.fromkeys(skills_found))\n",
    "    skills_found.sort()\n",
    "\n",
    "    return skills_found\n",
    "\n",
    "\n",
    "def extract_job_details(job_row, driver):\n",
    "   \n",
    "    try:\n",
    "        title_elem = job_row.find_element(By.CSS_SELECTOR, \"h2[itemprop='title']\")\n",
    "        title = title_elem.text.strip() if title_elem else \"\"\n",
    "\n",
    "        company_elem = job_row.find_element(By.CSS_SELECTOR, \"h3[itemprop='name']\")\n",
    "        company = company_elem.text.strip() if company_elem else \"\"\n",
    "\n",
    "        link_elem = job_row.find_element(By.CSS_SELECTOR, \"a[itemprop='url']\")\n",
    "        job_url = link_elem.get_attribute(\"href\") if link_elem else \"\"\n",
    "\n",
    "        tags = []\n",
    "        try:\n",
    "            tag_elems = job_row.find_elements(By.CSS_SELECTOR, \"td.tags h3, td.tags div.tag\")\n",
    "            for tag_elem in tag_elems:\n",
    "                tag_text = tag_elem.text.strip()\n",
    "                if tag_text:\n",
    "                    tags.append(tag_text)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        salary = \"\"\n",
    "        try:\n",
    "            salary_elem = job_row.find_elements(By.CSS_SELECTOR, \".location, .salary\")\n",
    "            for elem in salary_elem:\n",
    "                text = elem.text.strip()\n",
    "                if \"$\" in text or \"€\" in text or \"£\" in text or \"salary\" in text.lower():\n",
    "                    salary = text\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        date_posted = \"\"\n",
    "        try:\n",
    "            date_elem = job_row.find_element(By.CSS_SELECTOR, \"time\")\n",
    "            if date_elem:\n",
    "                date_posted = date_elem.get_attribute(\"datetime\") or date_elem.text.strip()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        location_text = \"Remote\"\n",
    "        try:\n",
    "            location_selectors = [\n",
    "                \".location:not(:has(*))\",\n",
    "                \".location span\",\n",
    "                \"td.location\",\n",
    "                \"div.location\"\n",
    "            ]\n",
    "\n",
    "            for selector in location_selectors:\n",
    "                try:\n",
    "                    loc_elems = job_row.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for elem in loc_elems:\n",
    "                        text = elem.text.strip()\n",
    "                        if (text and\n",
    "                                \"$\" not in text and\n",
    "                                \"€\" not in text and\n",
    "                                \"£\" not in text and\n",
    "                                \"salary\" not in text.lower()):\n",
    "                            location_text = text\n",
    "                            break\n",
    "                except:\n",
    "                    continue\n",
    "                if location_text != \"Remote\":\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        country, city = extract_location_details(location_text)\n",
    "        category = categorize_job(title, tags)\n",
    "        skills_list = extract_skills_from_tags(tags)\n",
    "\n",
    "        job_record = {\n",
    "            \"Job Title\": title,\n",
    "            \"Company Name\": company,\n",
    "            \"Job URL\": job_url,\n",
    "            \"Job Tags\": \", \".join(tags) if tags else \"\",\n",
    "            \"Salary Range\": salary,\n",
    "            \"Date Posted\": date_posted,\n",
    "            \"Location\": location_text,\n",
    "            \"Country\": country,\n",
    "            \"City\": city,\n",
    "            \"Job Category\": category,\n",
    "            \"Skills Found\": \", \".join(skills_list) if skills_list else \"None identified\",\n",
    "            \"Skills Count\": len(skills_list),\n",
    "            \"Key Skill 1\": skills_list[0] if len(skills_list) > 0 else \"\",\n",
    "            \"Key Skill 2\": skills_list[1] if len(skills_list) > 1 else \"\",\n",
    "            \"Key Skill 3\": skills_list[2] if len(skills_list) > 2 else \"\",\n",
    "            \"Key Skill 4\": skills_list[3] if len(skills_list) > 3 else \"\",\n",
    "            \"Key Skill 5\": skills_list[4] if len(skills_list) > 4 else \"\"\n",
    "        }\n",
    "\n",
    "        return job_record\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_single_page(driver, url, target_jobs=200):\n",
    "    \n",
    "    print(f\"  Loading: {url}\")\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"tr.job\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(f\"    No jobs found on {url}\")\n",
    "        return []\n",
    "\n",
    "    scroll_to_load_jobs(driver, target_jobs)\n",
    "\n",
    "    try:\n",
    "        job_rows = driver.find_elements(By.CSS_SELECTOR, \"tr.job\")\n",
    "    except:\n",
    "        print(\"    Could not find job rows\")\n",
    "        return []\n",
    "\n",
    "    print(f\"    Processing {len(job_rows)} jobs...\")\n",
    "\n",
    "    page_jobs = []\n",
    "    for i, job_row in enumerate(job_rows, 1):\n",
    "        try:\n",
    "            job_data = extract_job_details(job_row, driver)\n",
    "            if job_data:\n",
    "                page_jobs.append(job_data)\n",
    "\n",
    "                if i % 50 == 0:\n",
    "                    print(f\"      Processed {i} jobs...\")\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    print(f\"    Extracted {len(page_jobs)} jobs from this page\")\n",
    "    return page_jobs\n",
    "\n",
    "\n",
    "def scrape_multiple_categories(target_jobs=700):\n",
    "\n",
    "    driver = None\n",
    "    all_jobs = []\n",
    "    jobs_collected = 0\n",
    "\n",
    "\n",
    "    categories = [\n",
    "        (\"main\", \"https://remoteok.com\"),\n",
    "        (\"development\", \"https://remoteok.com/remote-dev-jobs\"),\n",
    "        (\"design\", \"https://remoteok.com/remote-design-jobs\"),\n",
    "        (\"marketing\", \"https://remoteok.com/remote-marketing-jobs\"),\n",
    "        (\"sales\", \"https://remoteok.com/remote-sales-jobs\"),\n",
    "        (\"product\", \"https://remoteok.com/remote-product-jobs\"),\n",
    "        (\"support\", \"https://remoteok.com/remote-customer-support-jobs\"),\n",
    "        (\"finance\", \"https://remoteok.com/remote-finance-jobs\"),\n",
    "        (\"writing\", \"https://remoteok.com/remote-writing-jobs\"),\n",
    "        (\"legal\", \"https://remoteok.com/remote-legal-jobs\"),\n",
    "        (\"all-jobs\", \"https://remoteok.com/remote-jobs\")\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        driver = setup_driver()\n",
    "\n",
    "        print(f\"\\nStarting comprehensive scraping for {target_jobs} jobs...\")\n",
    "        print(\"This will scrape multiple job categories.\")\n",
    "        print(\"Please wait while it processes each category...\\n\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for category_name, url in categories:\n",
    "            if jobs_collected >= target_jobs:\n",
    "                print(f\"\\nTarget reached! Collected {jobs_collected} jobs.\")\n",
    "                break\n",
    "\n",
    "            print(f\"\\n[{len(all_jobs) + 1}] Scraping {category_name.upper()} category...\")\n",
    "\n",
    "\n",
    "            category_jobs = scrape_single_page(driver, url, target_jobs=150)\n",
    "\n",
    "            if category_jobs:\n",
    "\n",
    "                existing_urls = {job['Job URL'] for job in all_jobs}\n",
    "                new_jobs = []\n",
    "\n",
    "                for job in category_jobs:\n",
    "                    if job['Job URL'] not in existing_urls:\n",
    "                        new_jobs.append(job)\n",
    "                        existing_urls.add(job['Job URL'])\n",
    "\n",
    "                all_jobs.extend(new_jobs)\n",
    "                jobs_collected = len(all_jobs)\n",
    "\n",
    "                print(f\"  Added {len(new_jobs)} new jobs from {category_name}\")\n",
    "                print(f\"  Total so far: {jobs_collected} jobs\")\n",
    "\n",
    "\n",
    "            if jobs_collected < target_jobs:\n",
    "                time.sleep(2)\n",
    "\n",
    "\n",
    "        if jobs_collected < target_jobs:\n",
    "            print(\"\\n\\nTrying search queries to get more jobs...\")\n",
    "\n",
    "            search_queries = [\n",
    "                \"python\", \"javascript\", \"react\", \"aws\", \"devops\",\n",
    "                \"data\", \"analyst\", \"engineer\", \"developer\", \"remote\"\n",
    "            ]\n",
    "\n",
    "            for query in search_queries:\n",
    "                if jobs_collected >= target_jobs:\n",
    "                    break\n",
    "\n",
    "                print(f\"\\nSearching for: '{query}'\")\n",
    "                search_url = f\"https://remoteok.com/?search={query}\"\n",
    "\n",
    "                search_jobs = scrape_single_page(driver, search_url, target_jobs=100)\n",
    "\n",
    "                if search_jobs:\n",
    "                    existing_urls = {job['Job URL'] for job in all_jobs}\n",
    "                    new_jobs = []\n",
    "\n",
    "                    for job in search_jobs:\n",
    "                        if job['Job URL'] not in existing_urls:\n",
    "                            new_jobs.append(job)\n",
    "                            existing_urls.add(job['Job URL'])\n",
    "\n",
    "                    all_jobs.extend(new_jobs)\n",
    "                    jobs_collected = len(all_jobs)\n",
    "\n",
    "                    print(f\"  Added {len(new_jobs)} new jobs from '{query}' search\")\n",
    "                    print(f\"  Total so far: {jobs_collected} jobs\")\n",
    "\n",
    "                time.sleep(2)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        minutes = int(elapsed_time // 60)\n",
    "        seconds = int(elapsed_time % 60)\n",
    "\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"COMPREHENSIVE SCRAPING COMPLETE\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        print(f\"Time taken: {minutes} minutes {seconds} seconds\")\n",
    "        print(f\"Target jobs: {target_jobs}\")\n",
    "        print(f\"Jobs collected: {jobs_collected}\")\n",
    "\n",
    "        if jobs_collected < target_jobs:\n",
    "            print(f\"\\nNote: Could only collect {jobs_collected} jobs.\")\n",
    "            print(\"RemoteOK may be limiting the number of visible jobs.\")\n",
    "            print(\"Try running again later or use the archived jobs feature.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during comprehensive scraping: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "    return all_jobs\n",
    "\n",
    "\n",
    "def save_data_to_excel(jobs_data, filename=\"remoteok_700plus_jobs.xlsx\"):\n",
    "\n",
    "    if not jobs_data:\n",
    "        print(\"No jobs data to save.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(jobs_data)\n",
    "\n",
    "\n",
    "    initial_count = len(df)\n",
    "    df.drop_duplicates(subset=[\"Job URL\"], inplace=True, keep='first')\n",
    "    final_count = len(df)\n",
    "\n",
    "    if initial_count != final_count:\n",
    "        print(f\"Removed {initial_count - final_count} duplicate jobs\")\n",
    "\n",
    "    print(f\"\\nSaving {len(df)} jobs to Excel...\")\n",
    "\n",
    "\n",
    "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "\n",
    "        df.to_excel(writer, sheet_name='All Jobs', index=False)\n",
    "\n",
    "\n",
    "        print(\"Creating summary sheets...\")\n",
    "\n",
    "\n",
    "        if 'Skills Found' in df.columns:\n",
    "            all_skills = []\n",
    "            for skills_str in df['Skills Found']:\n",
    "                if skills_str and skills_str != \"None identified\":\n",
    "                    skills = [s.strip() for s in skills_str.split(',')]\n",
    "                    all_skills.extend(skills)\n",
    "\n",
    "            if all_skills:\n",
    "                skills_series = pd.Series(all_skills)\n",
    "                skills_summary = skills_series.value_counts().reset_index()\n",
    "                skills_summary.columns = ['Skill Name', 'Job Count']\n",
    "                skills_summary['Percentage'] = (skills_summary['Job Count'] / len(df) * 100).round(2)\n",
    "                skills_summary.to_excel(writer, sheet_name='Skills Analysis', index=False)\n",
    "\n",
    "\n",
    "        if 'Job Category' in df.columns:\n",
    "            category_summary = df['Job Category'].value_counts().reset_index()\n",
    "            category_summary.columns = ['Job Category', 'Count']\n",
    "            category_summary['Percentage'] = (category_summary['Count'] / len(df) * 100).round(2)\n",
    "            category_summary.to_excel(writer, sheet_name='By Category', index=False)\n",
    "\n",
    "\n",
    "        if 'Country' in df.columns:\n",
    "            country_summary = df['Country'].value_counts().reset_index()\n",
    "            country_summary.columns = ['Country', 'Count']\n",
    "            country_summary['Percentage'] = (country_summary['Count'] / len(df) * 100).round(2)\n",
    "            country_summary.to_excel(writer, sheet_name='By Country', index=False)\n",
    "\n",
    "\n",
    "        if 'Skills Count' in df.columns:\n",
    "            high_skill_df = df[df['Skills Count'] >= 3].copy()\n",
    "            high_skill_df = high_skill_df.sort_values('Skills Count', ascending=False)\n",
    "            high_skill_df.to_excel(writer, sheet_name='High Skill Jobs', index=False)\n",
    "\n",
    "\n",
    "        if 'Date Posted' in df.columns:\n",
    "            try:\n",
    "                df['Date Posted'] = pd.to_datetime(df['Date Posted'], errors='coerce')\n",
    "                recent_df = df[df['Date Posted'] >= pd.Timestamp.now() - pd.Timedelta(days=30)]\n",
    "                if not recent_df.empty:\n",
    "                    recent_df.to_excel(writer, sheet_name='Recent Jobs', index=False)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        if 'Company Name' in df.columns:\n",
    "            company_summary = df['Company Name'].value_counts().reset_index()\n",
    "            company_summary.columns = ['Company Name', 'Job Count']\n",
    "            company_summary = company_summary.head(20)  # Top 20 companies\n",
    "            company_summary.to_excel(writer, sheet_name='Top Companies', index=False)\n",
    "\n",
    "    print(f\"\\nExcel file created: {filename}\")\n",
    "    print(f\"File size: {os.path.getsize(filename) / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"File location: {os.path.abspath(filename)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"REMOTEOK 700+ JOBS SCRAPER - MULTI-CATEGORY APPROACH\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    target_jobs = 700\n",
    "\n",
    "    print(f\"\\nThis scraper will collect jobs from multiple categories\")\n",
    "    print(f\"to reach the target of {target_jobs} jobs.\")\n",
    "    print(\"\\nEstimated time: 10-15 minutes\")\n",
    "    print(\"A Chrome browser will open and automatically navigate through categories.\")\n",
    "\n",
    "    input(\"\\nPress Enter to start scraping...\")\n",
    "\n",
    "\n",
    "    jobs_data = scrape_multiple_categories(target_jobs)\n",
    "\n",
    "    if jobs_data:\n",
    "\n",
    "        filename = \"remoteok_700plus_jobs.xlsx\"\n",
    "        df = save_data_to_excel(jobs_data, filename)\n",
    "\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"FINAL SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total unique jobs collected: {len(df)}\")\n",
    "\n",
    "        if len(df) > 0:\n",
    "\n",
    "            print(\"\\nKEY STATISTICS:\")\n",
    "\n",
    "            if 'Job Category' in df.columns:\n",
    "                print(\"\\nTop Job Categories:\")\n",
    "                top_categories = df['Job Category'].value_counts().head(5)\n",
    "                for category, count in top_categories.items():\n",
    "                    percentage = (count / len(df)) * 100\n",
    "                    print(f\"  - {category:<25} {count:>4} jobs ({percentage:.1f}%)\")\n",
    "\n",
    "            if 'Country' in df.columns:\n",
    "                print(\"\\nTop Countries:\")\n",
    "                top_countries = df['Country'].value_counts().head(5)\n",
    "                for country, count in top_countries.items():\n",
    "                    percentage = (count / len(df)) * 100\n",
    "                    print(f\"  - {country:<25} {count:>4} jobs ({percentage:.1f}%)\")\n",
    "\n",
    "            if 'Skills Found' in df.columns:\n",
    "                jobs_with_skills = df[df['Skills Found'] != \"None identified\"].shape[0]\n",
    "                print(f\"\\nJobs with identified skills: {jobs_with_skills} ({jobs_with_skills / len(df) * 100:.1f}%)\")\n",
    "\n",
    "\n",
    "                all_skills = []\n",
    "                for skills_str in df['Skills Found']:\n",
    "                    if skills_str and skills_str != \"None identified\":\n",
    "                        skills = [s.strip() for s in skills_str.split(',')]\n",
    "                        all_skills.extend(skills)\n",
    "\n",
    "                if all_skills:\n",
    "                    top_skills = pd.Series(all_skills).value_counts().head(5)\n",
    "                    print(\"\\nTop 5 Most Demanded Skills:\")\n",
    "                    for skill, count in top_skills.items():\n",
    "                        percentage = (count / len(df)) * 100\n",
    "                        print(f\"  - {skill:<20} {count:>4} jobs ({percentage:.1f}%)\")\n",
    "\n",
    "\n",
    "            print(\"\\nSAMPLE JOBS (first 3):\")\n",
    "            for i, (idx, row) in enumerate(df.head(3).iterrows(), 1):\n",
    "                print(f\"\\n{i}. {row.get('Job Title', 'N/A')}\")\n",
    "                print(f\"   Company: {row.get('Company Name', 'N/A')}\")\n",
    "                print(f\"   Category: {row.get('Job Category', 'N/A')}\")\n",
    "                print(f\"   Country: {row.get('Country', 'N/A')}\")\n",
    "                skills = row.get('Skills Found', '')\n",
    "                if skills and skills != \"None identified\":\n",
    "                    print(f\"   Skills: {skills[:60]}...\" if len(skills) > 60 else f\"   Skills: {skills}\")\n",
    "\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(\"NEXT STEPS:\")\n",
    "        print(\"1. Open the Excel file: remoteok_700plus_jobs.xlsx\")\n",
    "        print(\"2. Use the 'All Jobs' sheet for Power BI import\")\n",
    "        print(\"3. Use summary sheets for quick analysis\")\n",
    "        print(\"4. Filter and analyze the data as needed\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNo jobs were collected. Possible reasons:\")\n",
    "        print(\"1. Internet connection issue\")\n",
    "        print(\"2. RemoteOK website structure changed\")\n",
    "        print(\"3. Anti-bot measures detected\")\n",
    "        print(\"\\nTry running again or check your internet connection.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "db45bc83d35e568f"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
